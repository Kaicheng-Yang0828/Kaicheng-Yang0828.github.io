
# ðŸ’» Public Papers
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI2025</div><img src='_pages/includes/image_paper/CLIP-CID.jpg' alt="sym" width="400px" height="300px"></div></div>
<div class='paper-box-text' markdown="1">
  
[CLIP-CID: Efficient CLIP Distillation via Cluster-Instance Discrimination](https://arxiv.org/pdf/2408.09441) \\
**Kaicheng Yang\***, Tiancheng Gu\*, Xiang An, Haiqiang Jiang, Xiangzi Dai, Ziyong Feng, Weidong Cai, Jiankang Deng (**First author**)

[**Arxiv**](https://arxiv.org/pdf/2408.09441)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">WACV2025</div><img src='_pages/includes/image_paper/ORID.jpg' alt="sym" width="400px" height="300px"></div></div>
<div class='paper-box-text' markdown="1">


[ORID: Organ-Regional Information Driven Framework for Radiology Report Generation](https://arxiv.org/pdf/2411.13025) \\
Tiancheng Gu\*, **Kaicheng Yang\***, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai (**Oral**, **Equal First author**)

[**Arxiv**](https://arxiv.org/pdf/2411.13025)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP2024</div><img src='_pages/includes/image_paper/RWKV-CLIP.jpg' alt="sym" width="400px" height="300px"></div></div>
<div class='paper-box-text' markdown="1">

[RWKV-CLIP: A Robust Vision-Language Representation Learner](https://arxiv.org/abs/2406.06973) \\
Tiancheng Gu\*, **Kaicheng Yang\***, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, Jiankang Deng (**Equal First author**)

[**Arxiv**](https://arxiv.org/abs/2406.06973), [**Code**](https://github.com/deepglint/RWKV-CLIP)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV2024</div><img src='_pages/includes/image_paper/MLCD.jpg' alt="sym" width="400px" height="300px"></div></div>
<div class='paper-box-text' markdown="1">

[Multi-label Cluster Discrimination for Visual Representation Learning](https://arxiv.org/pdf/2407.17331) \\
Xiang An, **Kaicheng Yang**, Xiangzi Dai, Ziyong Feng, Jiankang Deng

[**Arxiv**](https://arxiv.org/abs/2407.17331), [**Code**](https://github.com/deepglint/unicom)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPRW2024</div><img src='_pages/includes/image_paper/LaPA.jpg' alt="sym" width="400px" height="300px"></div></div>
<div class='paper-box-text' markdown="1">

[LaPA: Latent Prompt Assist Model For Medical Visual Question Answering](https://arxiv.org/pdf/2404.13039) \\
Tiancheng Gu, **Kaicheng Yang**, Dongnan Liu, Weidong Cai

[**Arxiv**](https://arxiv.org/abs/2404.13039), [**Code**](https://github.com/GaryGuTC/LaPA_model)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV2023</div><img src='_pages/includes/image_paper/ALIP.jpg' alt="sym" width="400px" height="300px"></div></div>
<div class='paper-box-text' markdown="1">

[ALIP: Adaptive Language-Image Pre-training with Synthetic Caption](https://arxiv.org/abs/2308.08428) \\
**Kaicheng Yang**, Jiankang Deng, Xiang An, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, Tongliang Liu

[**Arxiv**](https://arxiv.org/abs/2308.08428), [**Code**](https://github.com/deepglint/ALIP)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR2023</div><img src='_pages/includes/image_paper/unicom.jpg' alt="sym" width="400px" height="300px"></div></div>
<div class='paper-box-text' markdown="1">

[Unicom: Universal and Compact Representation Learning for Image Retrieval](https://arxiv.org/abs/2304.05884) \\
Xiang An, Jiankang Deng, **Kaicheng Yang**, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, Tongliang Liu

[**Arxiv**](https://arxiv.org/abs/2304.05884), [**Code**](https://github.com/deepglint/unicom)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM2020</div><img src='_pages/includes/image_paper/CM-BERT.jpg' alt="sym" width="400px" height="300px"></div></div>
<div class='paper-box-text' markdown="1">

[CM-BERT: Cross-Modal BERT for Text-Audio Sentiment Analysis](https://dl.acm.org/doi/10.1145/3394171.3413690) \\
**Kaicheng Yang**, Hua Xu, Kai Gao (**Oral**)

[**Code**](https://github.com/thuiar/Cross-Modal-BERT)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL2020</div><img src='_pages/includes/image_paper/CH-SIMS.jpg' alt="sym" width="400px" height="300px"></div></div>
<div class='paper-box-text' markdown="1">

[CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality](https://aclanthology.org/2020.acl-main.343.pdf) \\
Wenmeng Yu, Hua Xu, Fanyang Meng, Yilin Zhu, Yixiao Ma, Jiele Wu, Jiyun Zou, **Kaicheng Yang**

[**Code**](https://github.com/thuiar/MMSA)
</div>
</div>



