
# üíª Public Papers
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM2025</div><img src='_pages/includes/image_paper/DeGLA.jpg' alt="sym" width="320px" height="240px"></div></div>
<div class='paper-box-text' markdown="1">
  
[Decoupled Global-Local Alignment for Improving Compositional Understanding](https://arxiv.org/pdf/2504.16801) \\
Xiaoxing Hu\*, **Kaicheng Yang\***, Jun Wang, Haoran Xu, Ziyong Feng, Yupei Wang (**Equal First author**)

[**Arxiv**](https://arxiv.org/pdf/2504.16801), [**ÂÖ¨‰ºóÂè∑**](https://mp.weixin.qq.com/s/VC3K96_tKrVbdBux0zsD2Q), [**GitHub**](https://github.com/xiaoxing2001/DeGLA), [**Project Websit**](https://xiaoxing2001.github.io/DeGLA.github.io/)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM2025</div><img src='_pages/includes/image_paper/UniME.jpg' alt="sym" width="320px" height="240px"></div></div>
<div class='paper-box-text' markdown="1">
  
[Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs](https://arxiv.org/abs/2504.17432) \\
Tiancheng Gu\*, **Kaicheng Yang\***, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, Jiankang Deng (**Equal First author**)

[**Arxiv**](https://arxiv.org/abs/2504.17432), [**ÂÖ¨‰ºóÂè∑**](https://mp.weixin.qq.com/s/d4huTt19EWBkvtzAHDI1uQ), [**GitHub**](https://github.com/deepglint/UniME), [**Project Websit**](https://garygutc.github.io/UniME/)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM2025</div><img src='_pages/includes/image_paper/RealSyn.jpg' alt="sym" width="320px" height="240px"></div></div>
<div class='paper-box-text' markdown="1">
  
[RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm](https://arxiv.org/pdf/2502.12513) \\
Tiancheng Gu\*, **Kaicheng Yang\***, Chaoyi Zhang, Yin Xie, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, Jiankang Deng (**Equal First author**)

[**Arxiv**](https://arxiv.org/pdf/2502.12513), [**ÂÖ¨‰ºóÂè∑**](https://mp.weixin.qq.com/s/CnuVZMSkYHpFwnfaLZ-3qA), [**GitHub**](https://github.com/deepglint/RealSyn), [**Project Websit**](https://garygutc.github.io/RealSyn/)
</div>
</div>

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">IROS2025</div><img src='_pages/includes/image_paper/DLOV-3D.jpg' alt="sym" width="400px" height="300px"></div></div>
<div class='paper-box-text' markdown="1">

Dual-Level Open-Vocabulary 3D Scene Representation for Instance-Aware Robot Navigation \\
Tianlu Zheng, **Kaicheng Yang\***, Yilong Dou, Ziyong Feng, Qichuan Ding

</div>
</div> -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI2025</div><img src='_pages/includes/image_paper/CLIP-CID.jpg' alt="sym" width="320px" height="240px"></div></div>
<div class='paper-box-text' markdown="1">
  
[CLIP-CID: Efficient CLIP Distillation via Cluster-Instance Discrimination](https://arxiv.org/pdf/2408.09441) \\
**Kaicheng Yang\***, Tiancheng Gu\*, Xiang An, Haiqiang Jiang, Xiangzi Dai, Ziyong Feng, Weidong Cai, Jiankang Deng (**First author**)

[**Arxiv**](https://arxiv.org/pdf/2408.09441), [**ÂÖ¨‰ºóÂè∑**](https://mp.weixin.qq.com/s/vb1-nm-PXSJD5Qt-qIifyg)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">WACV2025</div><img src='_pages/includes/image_paper/ORID.jpg' alt="sym" width="320px" height="240px"></div></div>
<div class='paper-box-text' markdown="1">


[ORID: Organ-Regional Information Driven Framework for Radiology Report Generation](https://arxiv.org/pdf/2411.13025) \\
Tiancheng Gu\*, **Kaicheng Yang\***, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai (**Oral**, **Equal First author**)

[**Arxiv**](https://arxiv.org/pdf/2411.13025)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP2024</div><img src='_pages/includes/image_paper/RWKV-CLIP.jpg' alt="sym" width="320px" height="240px"></div></div>
<div class='paper-box-text' markdown="1">

[RWKV-CLIP: A Robust Vision-Language Representation Learner](https://arxiv.org/abs/2406.06973) \\
Tiancheng Gu\*, **Kaicheng Yang\***, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, Jiankang Deng (**Equal First author**)

[**Arxiv**](https://arxiv.org/abs/2406.06973), [**Code**](https://github.com/deepglint/RWKV-CLIP), [**ÂÖ¨‰ºóÂè∑**](https://mp.weixin.qq.com/s/0sDHeN3QwF6AdFx3QyOvRw)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV2024</div><img src='_pages/includes/image_paper/MLCD.jpg' alt="sym" width="320px" height="240px"></div></div>
<div class='paper-box-text' markdown="1">

[Multi-label Cluster Discrimination for Visual Representation Learning](https://arxiv.org/pdf/2407.17331) \\
Xiang An, **Kaicheng Yang**, Xiangzi Dai, Ziyong Feng, Jiankang Deng

[**Arxiv**](https://arxiv.org/abs/2407.17331), [**Code**](https://github.com/deepglint/unicom), [**ÂÖ¨‰ºóÂè∑**](https://mp.weixin.qq.com/s/K2GbXh6GlV06-6Q4DYbPFg)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV2023</div><img src='_pages/includes/image_paper/ALIP.jpg' alt="sym" width="320px" height="240px"></div></div>
<div class='paper-box-text' markdown="1">

[ALIP: Adaptive Language-Image Pre-training with Synthetic Caption](https://arxiv.org/abs/2308.08428) \\
**Kaicheng Yang**, Jiankang Deng, Xiang An, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, Tongliang Liu

[**Arxiv**](https://arxiv.org/abs/2308.08428), [**Code**](https://github.com/deepglint/ALIP), [**ÂÖ¨‰ºóÂè∑**](https://mp.weixin.qq.com/s/lq_coUgV5A_h9RGvYhKaEA)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR2023</div><img src='_pages/includes/image_paper/unicom.jpg' alt="sym" width="320px" height="240px"></div></div>
<div class='paper-box-text' markdown="1">

[Unicom: Universal and Compact Representation Learning for Image Retrieval](https://arxiv.org/abs/2304.05884) \\
Xiang An, Jiankang Deng, **Kaicheng Yang**, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, Tongliang Liu

[**Arxiv**](https://arxiv.org/abs/2304.05884), [**Code**](https://github.com/deepglint/unicom), [**ÂÖ¨‰ºóÂè∑**](https://mp.weixin.qq.com/s/GmrbHexhaJ-3q4-2SW5Wig)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM2020</div><img src='_pages/includes/image_paper/CM-BERT.jpg' alt="sym" width="320px" height="240px"></div></div>
<div class='paper-box-text' markdown="1">

[CM-BERT: Cross-Modal BERT for Text-Audio Sentiment Analysis](https://dl.acm.org/doi/10.1145/3394171.3413690) \\
**Kaicheng Yang**, Hua Xu, Kai Gao (**Oral**)

[**Code**](https://github.com/thuiar/Cross-Modal-BERT)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL2020</div><img src='_pages/includes/image_paper/CH-SIMS.jpg' alt="sym" width="320px" height="240px"></div></div>
<div class='paper-box-text' markdown="1">

[CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality](https://aclanthology.org/2020.acl-main.343.pdf) \\
Wenmeng Yu, Hua Xu, Fanyang Meng, Yilin Zhu, Yixiao Ma, Jiele Wu, Jiyun Zou, **Kaicheng Yang**

[**Code**](https://github.com/thuiar/MMSA)
</div>
</div>



